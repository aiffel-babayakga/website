{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2201072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from fp_smalltargets.pt...\n",
      "✅ Found 4184 subset genes in checkpoint.\n",
      "Loading gene metadata from gene_metadata.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljm16\\AppData\\Local\\Temp\\ipykernel_35956\\402263024.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CKPT_PATH, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved vocab to fp_model_vocab.json\n",
      "   - Total vocab size: 4188\n",
      "   - Subset genes: 4184\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# [수정됨] 경로 수정: 역슬래시(\\) -> 슬래시(/)\n",
    "# ==============================================================================\n",
    "# 파일이 현재 폴더 기준 어디에 있는지 정확해야 합니다.\n",
    "# 만약 코드가 실행되는 위치가 최상위 폴더면 아래 경로가 맞습니다.\n",
    "CKPT_PATH = \"fp_smalltargets.pt\"  \n",
    "GENE_META_PATH = \"gene_metadata.parquet\"\n",
    "OUTPUT_VOCAB_PATH = \"fp_model_vocab.json\" \n",
    "\n",
    "# 2. 체크포인트 로드\n",
    "print(f\"Loading checkpoint from {CKPT_PATH}...\")\n",
    "\n",
    "# 경로 존재 여부 먼저 확인 (디버깅용)\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"❌ 파일을 찾을 수 없습니다: {os.path.abspath(CKPT_PATH)}\")\n",
    "if not os.path.exists(GENE_META_PATH):\n",
    "    raise FileNotFoundError(f\"❌ 파일을 찾을 수 없습니다: {os.path.abspath(GENE_META_PATH)}\")\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "extra = checkpoint.get(\"extra\", {})\n",
    "\n",
    "# 저장된 subset_token_ids 확인\n",
    "subset_ids = extra.get(\"subset_token_ids\", [])\n",
    "if not subset_ids:\n",
    "    raise ValueError(\"❌ 체크포인트에 subset_token_ids가 없습니다! 학습 코드를 확인하세요.\")\n",
    "print(f\"✅ Found {len(subset_ids)} subset genes in checkpoint.\")\n",
    "\n",
    "# 3. 전체 유전자 메타데이터 로드\n",
    "print(f\"Loading gene metadata from {GENE_META_PATH}...\")\n",
    "gene_df = pd.read_parquet(GENE_META_PATH)\n",
    "\n",
    "full_id_to_symbol = dict(zip(gene_df['token_id'], gene_df['gene_symbol']))\n",
    "\n",
    "# 4. 모델 전용 Vocab 생성\n",
    "special_tokens = extra.get(\"SPECIAL_TOKENS\", [\"[PAD]\", \"[CLS]\", \"[ORGAN]\", \"[MASK]\"])\n",
    "n_special = len(special_tokens)\n",
    "\n",
    "model_vocab = {}\n",
    "\n",
    "# (1) 스페셜 토큰 추가\n",
    "for i, token in enumerate(special_tokens):\n",
    "    model_vocab[token] = i\n",
    "\n",
    "# (2) 유전자 토큰 추가\n",
    "subset_genes_list = []\n",
    "for i, original_token_id in enumerate(subset_ids):\n",
    "    gene_symbol = full_id_to_symbol.get(original_token_id, f\"UNK_{original_token_id}\")\n",
    "    \n",
    "    model_id = n_special + i\n",
    "    model_vocab[gene_symbol] = model_id\n",
    "    \n",
    "    subset_genes_list.append({\n",
    "        \"gene_symbol\": gene_symbol,\n",
    "        \"original_token_id\": original_token_id,\n",
    "        \"model_token_id\": model_id\n",
    "    })\n",
    "\n",
    "# 5. 결과 저장\n",
    "output_data = {\n",
    "    \"vocab_map\": model_vocab,\n",
    "    \"subset_genes\": subset_genes_list,\n",
    "    \"config\": {\n",
    "        \"max_seq_len\": extra.get(\"MAX_SEQ_LEN\"),\n",
    "        \"d_model\": extra.get(\"D_MODEL\"),\n",
    "        \"vocab_size\": extra.get(\"VOCAB_SIZE\")\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_VOCAB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Successfully saved vocab to {OUTPUT_VOCAB_PATH}\")\n",
    "print(f\"   - Total vocab size: {len(model_vocab)}\")\n",
    "print(f\"   - Subset genes: {len(subset_genes_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
